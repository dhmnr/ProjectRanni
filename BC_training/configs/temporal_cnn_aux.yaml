# Temporal CNN Model Configuration with Auxiliary Task
# Auxiliary task: Predict NPC animation from vision (forces visual attack recognition)

dataset:
  path: "./dataset/margit_100_256x144.zarr"
  train_ratio: 0.8
  val_ratio: 0.2
  split_seed: 42
  normalize_frames: true
  use_state: true  # Include state features
  validate_episodes: true

  # Oversample rare actions (dodge/attack) to balance class distribution
  oversample:
    actions: ["dodge_roll/dash", "attack"]
    ratio: 10.0

# Temporal context settings
temporal:
  num_history_frames: 8   # Include 8 past frames (total 9 frames including current)
  num_action_history: 0
  frame_skip: 1           # Every frame (1 = no skip, 2 = every other frame)
  stack_states: true      # Stack states temporally (same as frames) instead of single current state

# Auxiliary task configuration
# Forces the visual encoder to learn attack recognition from pixels
auxiliary:
  use_npc_anim_prediction: true  # Enable NPC animation prediction from vision
  npc_anim_classes: 54           # Number of NPC animation classes (matches npc_vocab_size)
  loss_weight: 0.5               # Weight for auxiliary loss (total_loss = action_loss + weight * aux_loss)

# State preprocessing (same as hybrid_state)
state_preprocessing:
  normalize_resources: true
  compute_distances: true
  anim_embed_dim: 16

model:
  name: temporal_cnn
  # Frame processing mode: 'channel_stack' or 'shared_encoder'
  # channel_stack: Faster, fewer params, treats time as extra channels
  # shared_encoder: More flexible, explicit per-frame features
  frame_mode: channel_stack

  # CNN encoder (shared or for stacked channels)
  conv_features: [32, 64, 128, 256]

  # Action history encoder
  action_history_features: 64

  # State encoder (MLP)
  state_encoder_features: [64, 64]
  state_output_features: 64

  # Fusion dense layers
  dense_features: [512, 256]
  dropout_rate: 0.3
  use_batch_norm: true

# Attention mechanisms (optional)
# Requires: stack_states=true for state attention, shared_encoder mode for frame attention
attention:
  use_frame_attention: false   # Self-attention over temporal frame features
  use_state_attention: false   # Self-attention over temporal state features
  use_cross_attention: false   # Cross-attention between frames and states
  num_heads: 4                 # Number of attention heads
  head_dim: 32                 # Dimension per attention head

training:
  batch_size: 32
  num_epochs: 15
  learning_rate: 0.001
  weight_decay: 0.001
  lr_schedule:
    type: cosine
    warmup_epochs: 3
    min_lr: 0.00001
  use_class_weights: true
  label_smoothing: 0.0
  grad_clip_norm: 1.0
  checkpoint_dir: "./checkpoints/temporal_cnn_aux"
  save_every_n_epochs: 10
  keep_last_n_checkpoints: 3

  # BCE loss for main task
  loss:
    type: bce

evaluation:
  eval_every_n_epochs: 1
  metrics_threshold: 0.5
  onset_buffer_frames: 5   # Â±5 frames tolerance for buffered onset metrics (~167ms at 30fps)

logging:
  use_wandb: true
  wandb_project: "ProjectRanni-BC"
  wandb_entity: "dhmnr-projects"
  wandb_run_name: "temporal_cnn_aux"
  log_every_n_steps: 50
  log_gradients: false

system:
  seed: 42
  num_workers: 4
  pin_memory: false
